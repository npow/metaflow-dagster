"""
DagsterCompiler: transforms a Metaflow FlowGraph into a Dagster definitions file.

Key insight: in Metaflow, the 'start' step is special by NAME, not by type.
Its graph type is determined by what self.next() does:
  - start → single next()    → type='start'
  - start → next(a, b)       → type='split'
  - start → next(a, foreach) → type='foreach'

Generated file structure:
  1. Preamble + imports
  2. Constants   (FLOW_FILE, METAFLOW_TOP_ARGS, METAFLOW_STEP_ENV)
  3. Helpers     (_make_run_id, _run_init, _run_step, _get_foreach_splits)
  4. Config      (Pydantic Config for flow parameters)
  5. Ops         (one @op per flow step)
  6. Job         (@job composing ops)
  7. Schedule    (optional)
  8. Definitions
"""

import os
import sys
from textwrap import dedent, indent
from typing import Dict, List, Optional

from metaflow.decorators import flow_decorators
from metaflow.parameters import deploy_time_eval
from metaflow.util import dict_to_cli_options


# ──────────────────────────────────────────────────────────────────────────────
# Preamble template
# ──────────────────────────────────────────────────────────────────────────────

_PREAMBLE = '''\
"""
Dagster definitions generated by metaflow-dagster.
Flow   : {flow_name}
Job    : {job_name}
Source : {flow_file}

DO NOT EDIT — re-generate with:
    python {flow_basename} dagster create {output_file}
"""
import hashlib
import json
import os
import subprocess
import sys
from typing import Dict, List, Optional

from dagster import (
    Config,
    Definitions,
    DynamicOut,
    DynamicOutput,
    In,
    Nothing,
    OpExecutionContext,
    Out,
    Output,
    ScheduleDefinition,
    job,
    op,
    schedule,
)

# ── Flow constants ─────────────────────────────────────────────────────────────
FLOW_FILE = {flow_file!r}
FLOW_NAME = {flow_name!r}

# Metaflow CLI top-level flags embedded at compile time
METAFLOW_TOP_ARGS: List[str] = {top_args!r}

# Metaflow env-vars forwarded to every subprocess
METAFLOW_STEP_ENV: Dict[str, str] = {step_env!r}


# ── Execution helpers ──────────────────────────────────────────────────────────

def _make_run_id(dagster_run_id: str) -> str:
    """Derive a short, deterministic Metaflow run-id from Dagster\'s UUID."""
    return "dagster-" + hashlib.sha1(dagster_run_id.encode()).hexdigest()[:12]


def _build_env(extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:
    e = {{**os.environ, **METAFLOW_STEP_ENV}}
    if extra:
        e.update(extra)
    return e


def _run_cmd(
    context: OpExecutionContext,
    cmd: List[str],
    extra_env: Optional[Dict[str, str]] = None,
) -> None:
    context.log.info("$ " + " ".join(str(c) for c in cmd))
    result = subprocess.run(
        cmd,
        env=_build_env(extra_env),
        capture_output=True,
        text=True,
        cwd=os.path.dirname(FLOW_FILE) or ".",
    )
    if result.stdout.strip():
        context.log.info(result.stdout)
    if result.stderr.strip():
        context.log.info(result.stderr)
    if result.returncode != 0:
        raise Exception(
            f"Command failed (exit {{result.returncode}}):\\n{{result.stderr[-2000:]}}"
        )


def _run_init(
    context: OpExecutionContext,
    run_id: str,
    params_task_id: str,
    parameters: Optional[Dict] = None,
) -> str:
    """Create the _parameters task. Returns its full pathspec.

    Metaflow reads parameter overrides from METAFLOW_INIT_<NAME> env vars
    (not METAFLOW_PARAMETERS). Values must be raw strings — Metaflow applies
    its own type coercion when it reads the parameter from the _parameters task.
    """
    cmd = (
        [sys.executable, FLOW_FILE]
        + METAFLOW_TOP_ARGS
        + ["init", "--run-id", run_id, "--task-id", params_task_id]
    )
    extra: Dict[str, str] = {{}}
    for k, v in (parameters or {{}}).items():
        env_key = "METAFLOW_INIT_" + k.upper().replace("-", "_")
        extra[env_key] = str(v)
    _run_cmd(context, cmd, extra or None)
    return f"{{run_id}}/_parameters/{{params_task_id}}"


def _run_step(
    context: OpExecutionContext,
    step_name: str,
    run_id: str,
    input_paths: str,
    task_id: str,
    extra_args: Optional[List[str]] = None,
) -> str:
    """Execute one Metaflow step. Returns its full task pathspec."""
    cmd = (
        [sys.executable, FLOW_FILE]
        + METAFLOW_TOP_ARGS
        + [
            "step", step_name,
            "--run-id", run_id,
            "--task-id", task_id,
            "--retry-count", "0",
            "--max-user-code-retries", "0",
            "--input-paths", input_paths,
        ]
    )
    if extra_args:
        cmd.extend(extra_args)
    _run_cmd(context, cmd)
    return f"{{run_id}}/{{step_name}}/{{task_id}}"


def _get_foreach_splits(run_id: str, step_name: str, task_id: str) -> int:
    """Read the foreach cardinality directly from the local datastore files.

    Reads <ds_root>/<flow>/<run>/<step>/<task>/0.data.json to find the SHA of
    _foreach_num_splits, then decompresses the gzip+pickle blob.
    Bypasses the Metaflow metadata service to avoid import-time caching issues.
    """
    import gzip
    import pickle
    ds_root = next(
        (a.split("--datastore-root=")[1] for a in METAFLOW_TOP_ARGS if a.startswith("--datastore-root=")),
        os.environ.get("METAFLOW_DATASTORE_SYSROOT_LOCAL", ""),
    )
    try:
        data_json_path = os.path.join(ds_root, FLOW_NAME, run_id, step_name, task_id, "0.data.json")
        with open(data_json_path) as fh:
            data_map = json.load(fh)
        sha = data_map["objects"]["_foreach_num_splits"]
        blob_path = os.path.join(ds_root, FLOW_NAME, "data", sha[:2], sha)
        with gzip.open(blob_path, "rb") as fh:
            return int(pickle.load(fh))
    except Exception as exc:
        raise RuntimeError(
            f"Could not read foreach splits for {{step_name!r}}: {{exc}}"
        ) from exc


def _add_step_metadata(
    context: OpExecutionContext,
    task_path: str,
    output_name: Optional[str] = None,
) -> None:
    """Emit artifact-key metadata to the Dagster UI.

    Reads the 0.data.json index from the local datastore so Dagster can show
    which Metaflow artifact keys exist for this step. Does NOT load artifact
    values — they remain in the Metaflow datastore. Also emits a Python
    snippet showing how to retrieve each artifact via the Metaflow Task API.
    """
    from dagster import MetadataValue
    parts = task_path.split("/")
    if len(parts) < 3:
        return
    run_id, step_name, task_id = parts[0], parts[1], parts[2]
    ds_root = next(
        (a.split("--datastore-root=")[1] for a in METAFLOW_TOP_ARGS
         if a.startswith("--datastore-root=")),
        os.environ.get("METAFLOW_DATASTORE_SYSROOT_LOCAL", ""),
    )
    data_json_path = os.path.join(
        ds_root, FLOW_NAME, run_id, step_name, task_id, "0.data.json"
    )
    try:
        with open(data_json_path) as fh:
            objects = json.load(fh).get("objects", {{}})
    except Exception:
        return
    keys = [k for k in objects if not k.startswith("_")]
    if not keys:
        return
    pathspec = "/".join([FLOW_NAME, run_id, step_name, task_id])
    lines = ["from metaflow import Task", "task = Task(" + repr(pathspec) + ")"]
    lines += ["task[" + repr(k) + "].data  # " + k for k in keys]
    snippet = "\\n".join(lines)
    meta = {{
        "artifact_keys": MetadataValue.md(
            "**Metaflow artifacts:** " + ", ".join("`" + k + "`" for k in keys)
        ),
        "retrieve (copy ↓)": MetadataValue.text(snippet),
    }}
    kwargs = {{"output_name": output_name}} if output_name is not None else {{}}
    try:
        context.add_output_metadata(meta, **kwargs)
    except Exception:
        pass


'''


# ──────────────────────────────────────────────────────────────────────────────
# Compiler
# ──────────────────────────────────────────────────────────────────────────────

class DagsterCompiler:
    def __init__(
        self,
        job_name: str,
        graph,
        flow,
        flow_file: str,
        metadata,
        environment,
        flow_datastore,
        event_logger,
        monitor,
        tags: List[str],
        with_decorators: List[str],
        namespace: Optional[str],
        username: str,
        max_workers: int,
        step_env: Dict[str, str],
    ):
        self.job_name = job_name
        self.graph = graph
        self.flow = flow
        self.flow_file = flow_file
        self.metadata = metadata
        self.environment = environment
        self.flow_datastore = flow_datastore
        self.event_logger = event_logger
        self.monitor = monitor
        self.tags = tags
        self.with_decorators = with_decorators
        self.namespace = namespace
        self.username = username
        self.max_workers = max_workers
        self.step_env = step_env

        self.flow_name = flow.name
        self.parameters = self._extract_parameters()
        self.schedule = self._extract_schedule()

    # ── Public ─────────────────────────────────────────────────────────────────

    def compile(self) -> str:
        parts = [self._render_preamble()]
        config = self._render_config_class()
        if config:
            parts.append(config)
        for node in self._topological_order():
            parts.append(self._render_op(node))
        parts.append(self._render_job())
        if self.schedule:
            parts.append(self._render_schedule())
        parts.append(self._render_definitions())
        return "\n\n".join(p for p in parts if p.strip()) + "\n"

    # ── Preamble ───────────────────────────────────────────────────────────────

    def _render_preamble(self) -> str:
        return _PREAMBLE.format(
            flow_name=self.flow_name,
            job_name=self.job_name,
            flow_file=self.flow_file,
            flow_basename=os.path.basename(self.flow_file),
            output_file="<output.py>",
            top_args=self._build_top_args(),
            step_env=self.step_env,
        )

    def _build_top_args(self) -> List[str]:
        top_opts_dict: Dict = {}
        for deco in flow_decorators(self.flow):
            top_opts_dict.update(deco.get_top_level_options())
        top_opts = list(dict_to_cli_options(top_opts_dict))
        args = top_opts + [
            "--quiet",
            "--no-pylint",
            "--metadata=%s" % self.metadata.TYPE,
            "--environment=%s" % self.environment.TYPE,
            "--datastore=%s" % self.flow_datastore.TYPE,
            "--datastore-root=%s" % self.flow_datastore.datastore_root,
            "--event-logger=%s" % self.event_logger.TYPE,
            "--monitor=%s" % self.monitor.TYPE,
        ]
        if self.namespace:
            args.append("--namespace=%s" % self.namespace)
        return args

    # ── Config class ───────────────────────────────────────────────────────────

    def _extract_parameters(self) -> List[Dict]:
        params = []
        for var, param in self.flow._get_parameters():
            default = deploy_time_eval(param.kwargs.get("default"))
            params.append({
                "name": param.name,
                "var": var,
                "default": default,
                "help": param.kwargs.get("help", ""),
                "type": type(default).__name__ if default is not None else "str",
            })
        return params

    def _render_config_class(self) -> str:
        if not self.parameters:
            return ""
        lines = [f"class {self.flow_name}Config(Config):"]
        for p in self.parameters:
            default = p["default"]
            type_name = p["type"]
            if type_name == "bool":
                ann, val = "bool", default
            elif type_name == "int":
                ann, val = "int", default
            elif type_name == "float":
                ann, val = "float", default
            else:
                ann, val = "str", str(default) if default is not None else ""
            comment = f"  # {p['help']}" if p["help"] else ""
            lines.append(f"    {p['name']}: {ann} = {val!r}{comment}")
        return "\n".join(lines)

    # ── Op rendering ───────────────────────────────────────────────────────────

    def _topological_order(self):
        visited = set()
        queue = [self.graph["start"]]
        result = []
        while queue:
            node = queue.pop(0)
            if node.name in visited:
                continue
            visited.add(node.name)
            result.append(node)
            for child in node.out_funcs:
                queue.append(self.graph[child])
        return result

    def _op_name(self, step_name: str) -> str:
        return f"op_{step_name}"

    def _render_op(self, node) -> str:
        # The "start" step is special regardless of its graph type
        if node.name == "start":
            return self._render_start_op(node)
        ntype = node.type
        if ntype == "end":
            return self._render_end_op(node)
        elif ntype in ("linear",):
            return self._render_linear_op(node)
        elif ntype == "join":
            return self._render_join_op(node)
        elif ntype == "split":
            return self._render_split_op(node)
        elif ntype == "foreach":
            return self._render_foreach_op(node)
        else:
            raise NotImplementedError(f"Unsupported node type: {ntype!r} for step {node.name!r}")

    # start op (handles any start type) ───────────────────────────────────────

    def _render_start_op(self, node) -> str:
        """The start step always runs init+step; outputs depend on its graph type."""
        ntype = node.type  # 'start', 'split', or 'foreach'
        config_ann = f", config: {self.flow_name}Config" if self.parameters else ""
        param_dict = (
            "{" + ", ".join(f'"{p["name"]}": config.{p["name"]}' for p in self.parameters) + "}"
            if self.parameters else "{}"
        )
        tags_code = self._tags_args_code()

        # The common body: init then step
        common = (
            f'    run_id = _make_run_id(context.run_id)\n'
            f'    parameters = {param_dict}\n'
            f'    params_path = _run_init(context, run_id, "params", parameters)\n'
            f'    task_path = _run_step(\n'
            f'        context, "start", run_id, params_path, "1",\n'
            f'        extra_args={tags_code},\n'
            f'    )\n'
        )

        if ntype in ("start", "linear"):
            return (
                f'@op(out=Out(str))\n'
                f'def {self._op_name("start")}(context: OpExecutionContext{config_ann}) -> str:\n'
                + common
                + f'    _add_step_metadata(context, task_path)\n'
                + f'    return task_path\n'
            )
        elif ntype == "split":
            branches = node.out_funcs
            first_branch = branches[0]
            out_spec = "{" + ", ".join(f'"{b}": Out(str)' for b in branches) + "}"
            yields = "".join(
                f'    yield Output(task_path, output_name="{b}")\n' for b in branches
            )
            return (
                f'@op(out={out_spec})\n'
                f'def {self._op_name("start")}(context: OpExecutionContext{config_ann}):\n'
                + common
                + f'    _add_step_metadata(context, task_path, output_name="{first_branch}")\n'
                + yields
            )
        elif ntype == "foreach":
            return (
                f'@op(out=DynamicOut(str))\n'
                f'def {self._op_name("start")}(context: OpExecutionContext{config_ann}):\n'
                + common
                + f'    _add_step_metadata(context, task_path)\n'
                + f'    num_splits = _get_foreach_splits(run_id, "start", "1")\n'
                + f'    for _i in range(num_splits):\n'
                + f'        yield DynamicOutput(f"{{task_path}}//_i={{_i}}", mapping_key=str(_i))\n'
            )
        else:
            raise NotImplementedError(f"start step with type {ntype!r}")

    # end op ───────────────────────────────────────────────────────────────────

    def _render_end_op(self, node) -> str:
        in_name = node.in_funcs[0] if node.in_funcs else "upstream"
        extra = self._tags_args_code()
        return dedent(f"""\
            @op(ins={{"{in_name}": In(str)}})
            def {self._op_name(node.name)}(context: OpExecutionContext, {in_name}: str) -> None:
                run_id = {in_name}.split("/")[0]
                _run_step(context, "end", run_id, {in_name}, "1", extra_args={extra})
        """)

    # linear op ────────────────────────────────────────────────────────────────

    def _render_linear_op(self, node) -> str:
        parent_is_foreach = any(
            self.graph[p].type == "foreach" or (p == "start" and self.graph["start"].type == "foreach")
            for p in node.in_funcs
        )
        in_name = node.in_funcs[0] if node.in_funcs else "upstream"
        extra = self._tags_args_code()

        if parent_is_foreach:
            # Body step of a foreach: receives encoded task_path + split index
            return dedent(f"""\
                @op(ins={{"{in_name}": In(str)}}, out=Out(str))
                def {self._op_name(node.name)}(context: OpExecutionContext, {in_name}: str) -> str:
                    # {in_name} = "run_id/step/task_id//_i=N"
                    parts = {in_name}.split("//_i=")
                    base_path, split_index = parts[0], int(parts[1]) if len(parts) > 1 else 0
                    run_id = base_path.split("/")[0]
                    task_id = f"1-{{split_index}}"
                    task_path = _run_step(
                        context, "{node.name}", run_id, base_path, task_id,
                        extra_args=["--split-index", str(split_index)] + {extra},
                    )
                    _add_step_metadata(context, task_path)
                    return task_path
            """)
        else:
            return dedent(f"""\
                @op(ins={{"{in_name}": In(str)}}, out=Out(str))
                def {self._op_name(node.name)}(context: OpExecutionContext, {in_name}: str) -> str:
                    run_id = {in_name}.split("/")[0]
                    task_path = _run_step(context, "{node.name}", run_id, {in_name}, "1", extra_args={extra})
                    _add_step_metadata(context, task_path)
                    return task_path
            """)

    # split op ─────────────────────────────────────────────────────────────────

    def _render_split_op(self, node) -> str:
        in_name = node.in_funcs[0] if node.in_funcs else "upstream"
        branches = node.out_funcs
        first_branch = branches[0]
        out_spec = "{" + ", ".join(f'"{b}": Out(str)' for b in branches) + "}"
        extra = self._tags_args_code()
        yields = "\n    ".join(
            f'yield Output(task_path, output_name="{b}")' for b in branches
        )
        return dedent(f"""\
            @op(ins={{"{in_name}": In(str)}}, out={out_spec})
            def {self._op_name(node.name)}(context: OpExecutionContext, {in_name}: str):
                run_id = {in_name}.split("/")[0]
                task_path = _run_step(context, "{node.name}", run_id, {in_name}, "1", extra_args={extra})
                _add_step_metadata(context, task_path, output_name="{first_branch}")
                {yields}
        """)

    # join op ──────────────────────────────────────────────────────────────────

    def _render_join_op(self, node) -> str:
        is_foreach_join = bool(
            node.split_parents
            and (
                self.graph[node.split_parents[-1]].type == "foreach"
                or node.split_parents[-1] == "start" and self.graph["start"].type == "foreach"
            )
        )
        extra = self._tags_args_code()

        if is_foreach_join:
            # Receives collected DynamicOutputs as List[str]
            in_name = "foreach_results"
            return dedent(f"""\
                @op(ins={{"{in_name}": In(list)}}, out=Out(str))
                def {self._op_name(node.name)}(context: OpExecutionContext, {in_name}: list) -> str:
                    run_id = {in_name}[0].split("/")[0]
                    from metaflow.util import compress_list
                    clean = [{in_name}[i].split("//_i=")[0] for i in range(len({in_name}))]
                    input_paths = compress_list(clean)
                    task_path = _run_step(context, "{node.name}", run_id, input_paths, "1", extra_args={extra})
                    _add_step_metadata(context, task_path)
                    return task_path
            """)
        else:
            # Regular split-join: receives one input per branch
            ins_parts = [f'"{p}": In(str)' for p in node.in_funcs]
            ins_spec = "{" + ", ".join(ins_parts) + "}"
            args = ", ".join(node.in_funcs)
            paths_list = "[" + ", ".join(node.in_funcs) + "]"
            return dedent(f"""\
                @op(ins={ins_spec}, out=Out(str))
                def {self._op_name(node.name)}(context: OpExecutionContext, {args}) -> str:
                    run_id = {node.in_funcs[0]}.split("/")[0]
                    from metaflow.util import compress_list
                    input_paths = compress_list({paths_list})
                    task_path = _run_step(context, "{node.name}", run_id, input_paths, "1", extra_args={extra})
                    _add_step_metadata(context, task_path)
                    return task_path
            """)

    # foreach op ───────────────────────────────────────────────────────────────

    def _render_foreach_op(self, node) -> str:
        in_name = node.in_funcs[0] if node.in_funcs else "upstream"
        extra = self._tags_args_code()
        # Note: we use _i to avoid clashing with any user variable names
        # The f-string in generated code: we must use non-f-string concat to avoid
        # the compiler's own f-string from interpolating {_i} prematurely.
        return dedent(f"""\
            @op(ins={{"{in_name}": In(str)}}, out=DynamicOut(str))
            def {self._op_name(node.name)}(context: OpExecutionContext, {in_name}: str):
                run_id = {in_name}.split("/")[0]
                task_path = _run_step(context, "{node.name}", run_id, {in_name}, "1", extra_args={extra})
                _add_step_metadata(context, task_path)
                num_splits = _get_foreach_splits(run_id, "{node.name}", "1")
                for _i in range(num_splits):
                    yield DynamicOutput(task_path + "//_i=" + str(_i), mapping_key=str(_i))
        """)

    # ── Job body ───────────────────────────────────────────────────────────────

    def _render_job(self) -> str:
        body = self._generate_job_body()
        # Build manually — dedent(f"""...""") mishandles multi-line substitutions
        body_indented = "\n".join("    " + line for line in body.splitlines())
        return f"@job\ndef {self.job_name}():\n{body_indented}\n"

    def _generate_job_body(self) -> str:
        """
        Walk the graph BFS and emit assignment statements.
        var_map: step_name → Python expression for its output.
        """
        lines: List[str] = []
        var_map: Dict[str, str] = {}

        def _emit(node) -> None:
            if node.name in var_map:
                return
            # Ensure parents are emitted first (except start which has no parents)
            for p in node.in_funcs:
                if p not in var_map:
                    _emit(self.graph[p])

            op_call = self._op_name(node.name)
            ntype = node.type

            # ── start step (always no-arg call; outputs depend on type) ────────
            if node.name == "start":
                var = f"r_start"
                lines.append(f"{var} = {op_call}()")
                var_map["start"] = var
                if ntype == "split":
                    for branch in node.out_funcs:
                        var_map[f"_branch_{branch}"] = f'{var}.{branch}'
                # foreach: downstream uses .map() on var
                return

            # ── end step ────────────────────────────────────────────────────────
            if ntype == "end":
                pvar = self._resolve_input(node, var_map)
                lines.append(f"{op_call}({pvar})")
                var_map[node.name] = "_end"
                return

            # ── linear step ─────────────────────────────────────────────────────
            if ntype == "linear":
                parent_is_foreach = any(
                    self.graph[p].type == "foreach"
                    or (p == "start" and self.graph["start"].type == "foreach")
                    for p in node.in_funcs
                )
                if parent_is_foreach:
                    # foreach body: use .map()
                    foreach_parent = next(
                        p for p in node.in_funcs
                        if self.graph[p].type == "foreach"
                        or (p == "start" and self.graph["start"].type == "foreach")
                    )
                    dyn_var = var_map[foreach_parent]
                    var = f"r_{node.name}"
                    lines.append(f"{var} = {dyn_var}.map({op_call})")
                    var_map[node.name] = var
                else:
                    pvar = self._resolve_input(node, var_map)
                    var = f"r_{node.name}"
                    lines.append(f"{var} = {op_call}({pvar})")
                    var_map[node.name] = var
                return

            # ── split step ──────────────────────────────────────────────────────
            if ntype == "split":
                pvar = self._resolve_input(node, var_map)
                var = f"r_{node.name}"
                lines.append(f"{var} = {op_call}({pvar})")
                var_map[node.name] = var
                for branch in node.out_funcs:
                    var_map[f"_branch_{branch}"] = f'{var}.{branch}'
                return

            # ── foreach step ────────────────────────────────────────────────────
            if ntype == "foreach":
                pvar = self._resolve_input(node, var_map)
                var = f"r_{node.name}"
                lines.append(f"{var} = {op_call}({pvar})")
                var_map[node.name] = var
                return

            # ── join step ────────────────────────────────────────────────────────
            if ntype == "join":
                is_foreach_join = bool(
                    node.split_parents
                    and (
                        self.graph[node.split_parents[-1]].type == "foreach"
                        or (
                            node.split_parents[-1] == "start"
                            and self.graph["start"].type == "foreach"
                        )
                    )
                )
                if is_foreach_join:
                    # collect dynamic results
                    body_step = node.in_funcs[0]
                    dyn_var = var_map[body_step]
                    var = f"r_{node.name}"
                    lines.append(f"{var} = {op_call}({dyn_var}.collect())")
                    var_map[node.name] = var
                else:
                    # regular split-join: use the branch op outputs, not the split op outputs
                    in_vars = [
                        var_map.get(p, p)
                        for p in node.in_funcs
                    ]
                    args = ", ".join(in_vars)
                    var = f"r_{node.name}"
                    lines.append(f"{var} = {op_call}({args})")
                    var_map[node.name] = var
                return

        for node in self._topological_order():
            _emit(node)

        return "\n".join(lines) if lines else "pass"

    def _resolve_input(self, node, var_map: Dict[str, str]) -> str:
        """Get the Python expression for the first upstream input to this node."""
        parent = node.in_funcs[0] if node.in_funcs else None
        if parent is None:
            return "None"
        # Check if there's a named branch output for this step
        branch_key = f"_branch_{node.name}"
        if branch_key in var_map:
            return var_map[branch_key]
        return var_map.get(parent, parent)

    # ── Schedule ───────────────────────────────────────────────────────────────

    def _extract_schedule(self) -> Optional[str]:
        sched = self.flow._flow_decorators.get("schedule")
        if not sched:
            return None
        sched = sched[0]
        if sched.attributes.get("cron"):
            return sched.attributes["cron"]
        elif sched.attributes.get("daily"):
            return "0 0 * * *"
        elif sched.attributes.get("hourly"):
            return "0 * * * *"
        elif sched.attributes.get("weekly"):
            return "0 0 * * 0"
        return None

    def _render_schedule(self) -> str:
        return dedent(f"""\
            {self.job_name}_schedule = ScheduleDefinition(
                job={self.job_name},
                cron_schedule={self.schedule!r},
            )
        """)

    # ── Definitions ────────────────────────────────────────────────────────────

    def _render_definitions(self) -> str:
        sched_list = f"[{self.job_name}_schedule]" if self.schedule else "[]"
        return dedent(f"""\
            defs = Definitions(
                jobs=[{self.job_name}],
                schedules={sched_list},
            )
        """)

    # ── Utilities ──────────────────────────────────────────────────────────────

    def _tags_args_code(self) -> str:
        args = []
        for tag in self.tags:
            args += ["--tag", tag]
        for deco in self.with_decorators:
            args += [f"--with={deco}"]
        return repr(args)
